2025-09-23 20:49:55,712 - cwa.core.models - INFO - Loading model on Lambda Labs: microsoft/DialoGPT-small (Size: small)
2025-09-23 20:49:55,765 - cwa.core.models - INFO - Before loading - Lambda GPU 0: Allocated 0.00GB, Reserved 0.00GB
2025-09-23 20:49:55,766 - cwa.core.models - INFO - Before loading - Lambda GPU 1: Allocated 0.00GB, Reserved 0.00GB
2025-09-23 20:49:55,767 - cwa.core.models - INFO - Before loading - Lambda VM CPU Memory: 7.6% used (33.5GB / 442.7GB)
2025-09-23 20:49:55,771 - cwa.core.models - INFO - Lambda Labs GPU detected: NVIDIA H100 80GB HBM3
2025-09-23 20:49:55,772 - cwa.core.models - INFO - Loading tokenizer...
2025-09-23 20:49:56,077 - cwa.core.models - INFO - Multi-GPU Lambda setup detected - using auto device mapping
2025-09-23 20:49:56,078 - cwa.core.models - INFO - Loading model with Lambda Labs optimization: {'low_cpu_mem_usage': True, 'trust_remote_code': False, 'cache_dir': '/tmp/hf_cache', 'torch_dtype': torch.float16, 'device_map': 'auto', 'use_flash_attention_2': True}
2025-09-23 20:49:56,162 - cwa.core.models - WARNING - Failed to load as CausalLM, trying base model: GPT2LMHeadModel.__init__() got an unexpected keyword argument 'use_flash_attention_2'
2025-09-23 20:49:56,634 - cwa.core.models - INFO - After loading - Lambda GPU 0: Allocated 0.12GB, Reserved 0.19GB
2025-09-23 20:49:56,635 - cwa.core.models - INFO - After loading - Lambda GPU 1: Allocated 0.13GB, Reserved 0.15GB
2025-09-23 20:49:56,636 - cwa.core.models - INFO - After loading - Lambda VM CPU Memory: 7.6% used (33.7GB / 442.7GB)
2025-09-23 20:49:56,647 - cwa.core.models - INFO - Lambda Labs model loaded successfully: {'model_type': 'BaseModel', 'model_name': 'microsoft/DialoGPT-small', 'model_size_category': 'small', 'total_parameters': 124439808, 'trainable_parameters': 124439808, 'parameters_in_millions': 124.44, 'architecture': 'gpt2', 'hidden_size': 768, 'num_layers': 12, 'vocab_size': 50257, 'max_position_embeddings': 1024, 'device_placement': 'cuda:0', 'dtype': 'torch.float16', 'quantized': False}
2025-09-23 20:49:56,648 - accelerate.big_modeling - WARNING - You shouldn't move a model that is dispatched using accelerate hooks.
2025-09-23 20:49:56,655 - cwa.theory.information_geometry - INFO - Initialized InformationGeometricAnalyzer on cuda:0
2025-09-23 20:49:56,656 - cwa.research.super_weight_analyzer - INFO - Starting super weight extraction in super_weight_discovery mode
2025-09-23 20:49:56,657 - cwa.research.super_weight_analyzer - INFO - Detected 12 transformer layers in microsoft/DialoGPT-small
2025-09-23 20:49:56,657 - cwa.research.super_weight_analyzer - INFO - Early layer focus: targeting layers [0, 1, 2, 3]
2025-09-23 20:49:56,658 - cwa.research.super_weight_analyzer - INFO - Analyzing 4 target layers: [0, 1, 2, 3]
2025-09-23 20:49:56,659 - cwa.research.super_weight_analyzer - INFO - Monitoring activation magnitudes in target layers
2025-09-23 20:49:56,936 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:56,949 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:56,955 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:56,965 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:56,971 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:56,979 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,035 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,038 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,041 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,044 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,056 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,062 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,065 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,077 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,086 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,092 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,095 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,102 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,105 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,115 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,123 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,128 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,131 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,138 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,148 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,154 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,164 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,172 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,175 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,186 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,193 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,196 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,203 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,206 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,209 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,212 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,215 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,222 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,225 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,233 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,236 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,245 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,248 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,251 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,253 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,259 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,262 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,269 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,272 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,282 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,285 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,288 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,291 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,297 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,305 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,308 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,311 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,314 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,317 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,324 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,332 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,335 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,339 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,341 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,345 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,348 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,351 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,354 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,357 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,359 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,362 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,368 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,371 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,374 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,378 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,381 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,384 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,387 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,390 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,393 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,396 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,399 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,402 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,405 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,408 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,411 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,414 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,417 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,420 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,423 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,426 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,429 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,432 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,442 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,445 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,448 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,451 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,454 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,457 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,460 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,461 - cwa.research.super_weight_analyzer - INFO - Calculating Hessian-based sensitivity scores
2025-09-23 20:49:57,497 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,503 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,506 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,510 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,513 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,517 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,520 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,524 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,527 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,531 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,534 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,538 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,541 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,545 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,548 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,551 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,555 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,558 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,562 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,565 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,569 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,572 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,576 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,579 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,583 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,586 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,590 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,593 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,597 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,600 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,604 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,607 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,611 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,614 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,617 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,621 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,624 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,628 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,631 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,635 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,638 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,642 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,645 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,649 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,652 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,656 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,659 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,663 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,666 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,670 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,673 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,677 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,680 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,683 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,687 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,690 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,694 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,697 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,701 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,704 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,708 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,711 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,715 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,718 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,722 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,725 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,729 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,732 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,735 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,739 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,742 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,746 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,749 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,753 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,756 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,760 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,763 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,766 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,770 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,773 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,777 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,780 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,784 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,787 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,791 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,794 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,798 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,801 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,804 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,808 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,811 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,815 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,818 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,822 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,825 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,829 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,832 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,835 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,839 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,842 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,846 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,849 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,853 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,856 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,860 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,863 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,866 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,870 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,873 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,877 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,880 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,884 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,887 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,891 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,894 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,897 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,901 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,904 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,908 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,911 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,915 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,918 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,921 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,925 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,928 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,932 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,935 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,939 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,942 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,946 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,949 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,953 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,956 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,959 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,963 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,966 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,970 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,973 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,977 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,980 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,984 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,987 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,991 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,994 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:57,997 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,001 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,004 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,008 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,011 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,015 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,018 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,022 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,025 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,029 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,032 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,036 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,039 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,043 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,046 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,049 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,053 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,056 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,060 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,063 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,067 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,070 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,074 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,077 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,080 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,084 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,087 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,091 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,094 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,098 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,101 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,105 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,108 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,112 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,115 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,119 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,122 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,125 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,129 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,132 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,136 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,139 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,143 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,146 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,150 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,153 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,156 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,160 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,163 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,167 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,170 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,174 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,177 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,181 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,184 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,188 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,191 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,195 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,198 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,201 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,205 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,208 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,212 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,215 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,219 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,222 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,226 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,229 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,233 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,236 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,239 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,243 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,246 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,250 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,253 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,257 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,260 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,264 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,267 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,271 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,274 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,278 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,281 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,284 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,288 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,291 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,295 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,298 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,302 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,305 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,309 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,312 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,315 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,319 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,323 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,326 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,329 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,333 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,336 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,340 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,343 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,347 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,350 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,354 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,357 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,360 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,364 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,367 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,371 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,375 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,378 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,381 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,385 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,388 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,392 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,395 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,399 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,402 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,406 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,409 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,413 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,416 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,420 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,423 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,427 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,430 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,434 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,437 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,441 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,444 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,448 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,451 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,455 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,458 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,461 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,465 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,468 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,472 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,475 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,479 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,482 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,486 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,489 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,493 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,496 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,499 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,503 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,506 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,510 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,513 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,517 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,520 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,524 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,527 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,531 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,534 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,538 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,541 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,545 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,548 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,552 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,555 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,559 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,562 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,566 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,569 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,573 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,576 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,579 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,583 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,586 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,590 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,593 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,597 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,600 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,604 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,607 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,611 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,614 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,618 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,621 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,625 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,628 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,632 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,635 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,639 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,642 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,646 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,649 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,652 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,656 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,659 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,663 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,666 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,670 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,673 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,677 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,680 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,684 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,688 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,691 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,695 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,698 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,701 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,705 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,708 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,712 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,715 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,719 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,722 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,726 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,729 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,732 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,736 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,739 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,743 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,746 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,750 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,753 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,757 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,760 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,764 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,767 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,770 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,774 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,777 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,781 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,784 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,788 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,791 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,795 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,798 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,802 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,805 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,808 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,812 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,815 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,819 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,822 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,826 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,829 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,833 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,836 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,840 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,843 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,847 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,850 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,854 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,857 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,861 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,864 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,867 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,871 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,874 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,878 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,881 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,885 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,888 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,892 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,895 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,899 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,902 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,906 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,909 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,913 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,916 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,920 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,923 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,927 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,930 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,934 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,937 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,940 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,944 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,947 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,951 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,954 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,958 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,961 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,965 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,968 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,972 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,975 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,979 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,982 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,986 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,989 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,993 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:58,996 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,000 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,003 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,006 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,010 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,013 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,017 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,020 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,024 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,027 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,031 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,034 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,038 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,041 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,044 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,048 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,051 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,055 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,058 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,062 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,065 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,069 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,072 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,076 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,079 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,082 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,086 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,089 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,093 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,096 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,100 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,103 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,107 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,110 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,113 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,117 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,120 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,124 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,127 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,131 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,134 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,138 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,141 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,145 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,148 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,151 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,155 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,158 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,162 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,165 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,169 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,172 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,176 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,179 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,183 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,186 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,189 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,193 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,196 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,200 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,203 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,207 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,210 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,214 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,217 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,221 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,224 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,227 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,231 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,234 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,238 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,241 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,245 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,248 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,252 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,255 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,258 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,262 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,265 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,269 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,272 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,276 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,279 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,283 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,286 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,290 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,293 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,296 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,300 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,303 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,307 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,310 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,314 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,317 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,321 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,324 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,328 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,331 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,335 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,338 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,341 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,345 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,348 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,352 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,355 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,359 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,362 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,366 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,369 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,373 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,376 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,379 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,383 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,386 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,390 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,393 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,397 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,400 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,404 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,407 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,411 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,414 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,417 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,421 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,424 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,428 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,431 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,435 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,438 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,442 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,445 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,449 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,452 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,456 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,459 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,462 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,466 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,470 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,473 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,476 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,480 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,483 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,487 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,490 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,494 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,497 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,501 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,504 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,508 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,511 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,514 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,518 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,521 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,525 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,528 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,532 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,535 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,539 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,542 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,546 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,549 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,552 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,556 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,559 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,563 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,566 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,570 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,573 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,577 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,586 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,590 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,593 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,596 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,612 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,615 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,619 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,622 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,634 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,638 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,641 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,645 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,658 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,662 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,665 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,668 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,680 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,683 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,687 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,690 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,704 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,707 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,711 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,714 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,727 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,731 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,734 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,737 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,749 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,753 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,756 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,760 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,763 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,767 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,770 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,785 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,788 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,792 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,795 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,807 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,811 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,814 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,818 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,828 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,832 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,835 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,839 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,849 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,853 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,856 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,860 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,864 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,868 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,871 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,875 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,882 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,885 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,889 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,892 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,896 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,899 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,903 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,906 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,916 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,920 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,923 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,927 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,936 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,940 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,943 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,947 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,956 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,960 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,963 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,967 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,981 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,985 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,988 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:49:59,992 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,005 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,008 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,012 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,015 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,028 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,032 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,035 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,039 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,049 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,053 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,056 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,060 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,072 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,076 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,079 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,083 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,096 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,100 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,103 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,106 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,119 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,122 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,126 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,129 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,142 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,145 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,148 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,152 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,163 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,166 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,170 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,173 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,186 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,189 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,193 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,196 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,206 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,210 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,213 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,217 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,227 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,230 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,234 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,237 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,245 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,248 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,252 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,255 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,259 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,262 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,266 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,269 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,273 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,276 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,279 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,283 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,286 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,290 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,293 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,297 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,300 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,304 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,307 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,311 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,314 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,318 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,321 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,324 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,328 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,331 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,335 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,338 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,342 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,345 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,348 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,352 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,355 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,359 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,362 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,366 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,369 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,373 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,376 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,380 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,383 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,386 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,390 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,393 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,397 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,400 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,404 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,407 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,411 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,414 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,417 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,421 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,424 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,428 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,431 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,435 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,438 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,442 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,445 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,449 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,452 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,456 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,459 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,463 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,466 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,470 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,473 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,476 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,480 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,483 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,487 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,490 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,494 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,497 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,501 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,504 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,508 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,511 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,515 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,518 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,521 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,525 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,528 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,532 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,535 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,539 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,542 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,546 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,549 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,553 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,556 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,560 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,563 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,566 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,570 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,573 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,577 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,580 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,584 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,587 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,591 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,594 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,597 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,601 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,604 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,608 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,611 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,615 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,618 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,622 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,625 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,629 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,632 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,636 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,639 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,643 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,646 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,649 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,653 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,656 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,660 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,663 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,667 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,670 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,674 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,677 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,680 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,684 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,687 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,691 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,694 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,698 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,701 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,705 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,708 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,712 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,715 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,719 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,722 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,726 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,729 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,732 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,736 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,739 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,743 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,746 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,750 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,753 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,757 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,760 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,764 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,767 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,771 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,774 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,777 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,781 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,784 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,788 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,791 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,795 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,798 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,802 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,805 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,808 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,812 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,815 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,819 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,822 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,826 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,829 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,833 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,836 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,839 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,843 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,846 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,850 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,853 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,857 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,860 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,864 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,867 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,871 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,874 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,878 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,881 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,884 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,888 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,891 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,895 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,898 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,902 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,905 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,909 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,912 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,916 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,919 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,923 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,926 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,930 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,933 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,936 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,940 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,943 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,947 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,950 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,954 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,957 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,961 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,964 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,968 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,971 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,975 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,978 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,981 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,985 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,988 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,992 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,995 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:00,999 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,002 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,005 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,009 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,012 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,016 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,019 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,023 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,026 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,030 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,033 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,037 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,040 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,044 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,047 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,051 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,054 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,057 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,061 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,064 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,068 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,071 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,075 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,078 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,082 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,085 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,089 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,092 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,095 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,099 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,102 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,106 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,109 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,113 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,116 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,120 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,123 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,127 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,130 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,133 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,137 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,140 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,144 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,147 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,151 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,154 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,158 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,161 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,164 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,168 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,171 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,175 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,178 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,182 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,185 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,189 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,192 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,196 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,199 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,203 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,206 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,209 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,213 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,216 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,220 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,223 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,227 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,230 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,234 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,237 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,240 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,244 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,247 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,251 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,254 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,258 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,261 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,265 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,268 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,272 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,275 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,279 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,282 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,286 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,289 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,292 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,296 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,299 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,303 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,306 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,310 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,313 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,317 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,320 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,324 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,327 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,331 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,334 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,337 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,341 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,344 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,348 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,351 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,355 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,358 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,362 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,365 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,369 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,372 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,375 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,379 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,382 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,386 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,389 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,393 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,396 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,400 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,403 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,407 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,410 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,413 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,417 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,420 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,424 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,427 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,431 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,434 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,438 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,441 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,444 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,448 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,451 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,455 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,458 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,462 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,465 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,469 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,472 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,476 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,479 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,483 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,486 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,490 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,493 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,496 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,500 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,503 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,507 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,510 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,514 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,517 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,521 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,524 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,528 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,531 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,535 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,538 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,541 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,545 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,548 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,552 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,555 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,559 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,562 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,566 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,569 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,572 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,576 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,579 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,583 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,586 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,590 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,593 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,597 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,600 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,604 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,607 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,611 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,614 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,617 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,621 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,624 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,628 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,631 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,635 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,638 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,642 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,645 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,648 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,652 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,655 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,659 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,662 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,666 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,669 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,673 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,676 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,680 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,683 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,686 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,690 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,693 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,697 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,700 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,704 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,707 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,711 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,714 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,717 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,721 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,724 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,728 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,731 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,735 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,738 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,741 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,745 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,748 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,752 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,755 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,759 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,762 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,766 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,769 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,772 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,776 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,779 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,783 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,786 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,790 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,793 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,797 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,800 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,803 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,807 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,810 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,814 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,817 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,821 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,824 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,828 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,831 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,834 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,838 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,841 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,845 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,848 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,852 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,855 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,859 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,862 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,866 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,869 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,872 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,876 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,879 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,883 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,886 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,890 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,893 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,896 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,900 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,903 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,907 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,910 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,914 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,917 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,921 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,924 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,928 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,931 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,934 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,938 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,941 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,945 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,948 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,952 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,955 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,959 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,962 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,965 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,969 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,972 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,976 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,979 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,983 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,986 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,990 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,993 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:01,996 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,003 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,007 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,010 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,014 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,017 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,021 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,024 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,028 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,031 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,035 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,038 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,041 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,045 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,048 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,052 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,055 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,059 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,062 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,066 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,069 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,073 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,076 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,079 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,083 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,086 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,090 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,093 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,097 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,100 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,104 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,107 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,111 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,114 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,117 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,121 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,124 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,128 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,131 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,135 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,138 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,141 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,145 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,148 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,152 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,155 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,159 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,162 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,166 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,169 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,172 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,176 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,179 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,183 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,186 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,190 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,193 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,197 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,200 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,204 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,207 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,210 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,214 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,217 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,221 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,224 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,228 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,231 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,235 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,238 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,242 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,245 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,248 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,252 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,255 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,259 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,262 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,266 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,269 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,273 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,276 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,279 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,283 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,286 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,290 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,293 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,297 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,300 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,304 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,307 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,311 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,314 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,318 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,321 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,324 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,328 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,331 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,335 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,338 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,342 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,345 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,349 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,352 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,356 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,359 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,362 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,366 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,369 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,373 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,376 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,380 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,383 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,387 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,390 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,394 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,397 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,400 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,404 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,407 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,411 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,414 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,418 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,421 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,425 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,428 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,432 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,435 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,439 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,442 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,446 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,449 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,452 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,456 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,459 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,463 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,466 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,470 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,473 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,477 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,480 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,484 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,487 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,491 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,494 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,497 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,501 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,504 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,508 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,511 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,515 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,518 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,522 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,525 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,529 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,532 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,536 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,539 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,543 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,546 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,549 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,553 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,557 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,560 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,563 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,567 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,570 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,574 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,577 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,581 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,584 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,588 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,591 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,595 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,598 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,602 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,605 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,609 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,612 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,615 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,619 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,622 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,626 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,629 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,633 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,636 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,640 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,643 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,647 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,650 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,654 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,657 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,661 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,664 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,668 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,671 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,674 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,678 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,681 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,685 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,688 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,692 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,695 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,699 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,702 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,706 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,709 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,713 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,716 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,720 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,723 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,726 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,730 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,733 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,737 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,740 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,744 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,747 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,751 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,754 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,758 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,761 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,765 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,768 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,772 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,775 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,779 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,782 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,785 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,789 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,792 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,796 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,799 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,803 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,806 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,810 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,813 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,817 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,820 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,824 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,827 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,831 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,834 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,837 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,841 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,844 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,848 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,851 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,855 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,858 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,862 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,865 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,869 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,872 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,876 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,879 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,883 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,886 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,889 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,893 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,896 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,900 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,903 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,907 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,910 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,914 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,917 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,921 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,924 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,928 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,931 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,935 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,938 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,941 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,945 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,948 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,952 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,955 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,959 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,962 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,966 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,969 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,973 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,976 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,980 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,983 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,987 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,990 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,993 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:02,997 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,001 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,004 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,008 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,011 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,014 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,018 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,021 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,025 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,028 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,032 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,035 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,039 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,042 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,046 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,049 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,053 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,056 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,060 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,063 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,067 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,070 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,073 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,077 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,080 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,084 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,087 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,091 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,094 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,098 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,101 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,105 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,108 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,112 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,115 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,118 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,122 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,125 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,129 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,132 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,136 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,139 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,143 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,146 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,150 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,153 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,156 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,160 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,163 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,167 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,170 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,174 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,177 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,181 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,184 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,188 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,191 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,195 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,198 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,201 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,205 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,208 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,212 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,215 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,219 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,222 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,226 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,229 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,232 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,236 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,239 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,243 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,246 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,250 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,253 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,257 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,260 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,264 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,267 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,271 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,274 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,277 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,281 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,284 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,288 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,291 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,295 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,298 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,302 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,305 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,308 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,312 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,315 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,319 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,322 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,326 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,329 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,333 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,336 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,340 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,343 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,346 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,350 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,353 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,357 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,360 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,364 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,367 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,371 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,374 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,378 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,381 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,385 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,388 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,391 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,395 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,398 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,402 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,405 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,409 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,412 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,416 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,419 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,423 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,426 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,430 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,433 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,436 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,440 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,443 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,447 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,450 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,454 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,457 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,461 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,464 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,468 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,471 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,475 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,478 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,481 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,485 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,488 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,492 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,495 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,499 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,502 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,506 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,509 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,513 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,516 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,520 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,523 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,526 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,530 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,533 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,537 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,540 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,544 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,547 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,551 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,554 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,558 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,561 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,564 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,568 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,571 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,575 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,578 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,582 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,585 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,589 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,592 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,596 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,599 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,602 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,606 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,609 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,613 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,616 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,620 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,623 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,627 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,630 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,634 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,637 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,640 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,644 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,647 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,651 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,654 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,658 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,661 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,665 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,668 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,672 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,675 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,679 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,682 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,685 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,689 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,692 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,696 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,699 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,703 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,706 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,710 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,713 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,717 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,720 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,723 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,727 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,730 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,734 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,737 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,741 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,744 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,748 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,751 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,755 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,758 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,761 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,765 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,768 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,772 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,775 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,779 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,782 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,786 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,789 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,793 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,796 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,799 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,803 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,806 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,810 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,813 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,817 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,820 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,824 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,827 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,831 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,834 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,838 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,841 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,844 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,848 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,851 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,855 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,858 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,862 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,865 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,869 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,872 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,876 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,879 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,883 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,886 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,890 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,893 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,896 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,900 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,903 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,907 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,910 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,914 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,917 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,921 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,924 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,928 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,931 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,935 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,938 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,941 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,945 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,948 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,952 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,955 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,959 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,962 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,966 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,969 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,973 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,976 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,980 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,983 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,986 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,990 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,993 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:03,997 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,000 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,004 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,007 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,011 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,014 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,018 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,021 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,024 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,028 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,031 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,035 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,038 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,042 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,045 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,049 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,052 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,056 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,059 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,063 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,066 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,070 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,073 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,076 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,080 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,083 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,087 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,090 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,094 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,097 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,101 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,104 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,108 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,111 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,115 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,118 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,121 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,125 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,128 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,132 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,135 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,139 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,142 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,146 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,149 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,153 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,156 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,160 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,163 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,166 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,170 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,173 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,177 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,180 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,184 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,187 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,191 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,194 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,197 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,201 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,204 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,208 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,211 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,215 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,218 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,222 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,225 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,229 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,232 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,235 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,239 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,242 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,246 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,249 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,253 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,256 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,260 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,263 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,267 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,270 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,273 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,277 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,280 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,284 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,287 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,291 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,294 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,298 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,301 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,305 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,308 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,312 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,315 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,318 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,322 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,325 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,329 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,332 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,336 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,339 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,343 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,346 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,349 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,353 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,356 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,360 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,363 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,367 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,370 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,374 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,377 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,380 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,384 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,387 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,391 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,394 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,398 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,401 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,405 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,408 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,412 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,415 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,419 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,422 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,425 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,429 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,432 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,436 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,439 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,443 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,446 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,450 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,453 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,457 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,460 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,463 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,467 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,470 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,474 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,477 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,481 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,484 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,488 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,491 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,494 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,498 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,501 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,505 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,508 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,512 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,515 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,519 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,522 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,526 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,529 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,533 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,536 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,540 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,543 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,546 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,550 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,553 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,557 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,560 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,564 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,567 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,571 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,574 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,577 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,581 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,584 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,588 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,591 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,595 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,598 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,602 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,605 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,609 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,612 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,616 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,619 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,622 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,626 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,629 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,633 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,636 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,640 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,643 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,647 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,650 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,653 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,657 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,660 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,758 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,763 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,766 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,770 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,773 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,777 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,780 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,784 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,787 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,791 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,794 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,798 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,801 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,805 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,808 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,812 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,815 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,818 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,822 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,825 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,829 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,832 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,836 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,839 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,843 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,846 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,850 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,853 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,856 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,860 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,863 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,867 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,870 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,874 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,877 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,881 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,884 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,888 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,891 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,895 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,898 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,902 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,905 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,909 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,912 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,916 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,919 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,922 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,926 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,929 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,933 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,936 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,940 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,943 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,947 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,950 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,954 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,957 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,961 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,964 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,968 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,971 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,974 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,978 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,981 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,985 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,988 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,992 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,995 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:04,999 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,002 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,006 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,009 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,013 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,016 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,020 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,023 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,026 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,030 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,033 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,037 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,040 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,044 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,047 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,051 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,054 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,058 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,061 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,065 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,068 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,072 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,075 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,078 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,082 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,085 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,089 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,092 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,096 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,099 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,103 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,106 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,110 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,113 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,117 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,120 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,124 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,127 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,130 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,134 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,137 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,141 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,144 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,148 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,151 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,155 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,158 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,162 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,165 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,169 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,172 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,175 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,179 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,182 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,186 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,189 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,193 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,196 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,200 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,203 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,207 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,210 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,214 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,217 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,220 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,224 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,227 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,231 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,234 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,238 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,241 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,245 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,248 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,252 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,255 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,259 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,262 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,266 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,269 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,273 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,276 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,280 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,283 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,286 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,290 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,293 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,297 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,300 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,304 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,307 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,311 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,314 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,318 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,321 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,325 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,328 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,332 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,335 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,339 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,342 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,345 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,349 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,352 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,356 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,359 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,363 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,366 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,370 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,373 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,377 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,380 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,384 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,387 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,391 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,394 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,398 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,401 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,404 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,408 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,411 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,415 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,418 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,422 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,425 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,429 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,432 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,436 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,439 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,443 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,446 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,449 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,453 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,456 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,460 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,463 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,467 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,470 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,474 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,477 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,481 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,484 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,487 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,491 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,495 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,498 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,501 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,505 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,508 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,512 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,515 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,519 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,522 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,526 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,529 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,533 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,536 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,540 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,543 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,547 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,550 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,553 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,557 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,560 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,564 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,567 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,571 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,574 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,578 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,581 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,585 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,588 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,592 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,595 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,599 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,602 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,606 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,609 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,612 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,616 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,619 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,623 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,626 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,630 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,633 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,637 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,640 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,644 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,647 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,651 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,654 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,658 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,661 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,664 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,668 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,671 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,675 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,678 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,682 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,685 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,689 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,692 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,696 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,699 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,703 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,706 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,710 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,713 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,717 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,720 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,724 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,727 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,730 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,734 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,737 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,741 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,744 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,748 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,751 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,755 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,758 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,762 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,765 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,769 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,772 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,776 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,779 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,782 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,786 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,789 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,793 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,796 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,800 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,803 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,807 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,810 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,814 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,817 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,820 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,824 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,827 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,831 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,834 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,838 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,841 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,845 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,848 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,852 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,855 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,859 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,862 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,865 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,869 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,872 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,876 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,879 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,883 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,886 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,890 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,893 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,896 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,900 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,903 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,907 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,910 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,914 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,917 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,920 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,924 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,927 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,931 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,934 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,938 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,941 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,945 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,948 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,952 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,955 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,958 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,962 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,965 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,969 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,972 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,976 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,979 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,983 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,986 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,990 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,993 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:05,997 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,000 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,004 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,007 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,010 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,014 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,017 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,021 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,024 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,028 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,031 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,035 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,038 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,042 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,045 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,048 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,052 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,055 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,059 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,062 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,066 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,069 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,073 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,076 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,080 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,083 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,087 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,090 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,094 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,097 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,101 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,104 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,108 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,111 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,114 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,118 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,121 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,125 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,128 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,132 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,135 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,139 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,142 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,146 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,149 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,153 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,156 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,160 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,163 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,166 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,170 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,173 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,177 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,180 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,184 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,187 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,191 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,194 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,198 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,201 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,204 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,208 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,211 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,215 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,218 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,222 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,225 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,229 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,232 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,236 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,239 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,243 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,246 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,250 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,253 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,256 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,260 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,263 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,267 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,270 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,274 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,277 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,281 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,284 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,288 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,291 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,295 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,298 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,302 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,305 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,309 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,312 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,315 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,319 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,322 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,326 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,329 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,333 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,336 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,340 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,343 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,347 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,350 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,354 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,357 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,361 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,364 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,368 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,371 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,375 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,378 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,381 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,385 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,388 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,392 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,395 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,399 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,402 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,406 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,409 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,413 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,416 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,420 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,423 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,427 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,430 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,434 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,437 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,440 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,444 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,447 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,451 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,454 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,458 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,461 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,465 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,468 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,472 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,475 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,479 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,482 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,485 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,489 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,492 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,496 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,499 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,503 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,506 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,510 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,513 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,517 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,520 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,524 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,527 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,531 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,534 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,537 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,541 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,544 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,548 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,551 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,555 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,558 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,562 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,565 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,569 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,572 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,576 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,579 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,583 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,586 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,589 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,593 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,596 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,600 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,603 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,607 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,610 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,614 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,617 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,621 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,624 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,628 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,631 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,635 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,638 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,642 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,645 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,649 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,652 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,656 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,659 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,662 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,666 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,669 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,673 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,676 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,680 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,683 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,687 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,690 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,694 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,697 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,701 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,704 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,708 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,711 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,715 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,718 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,721 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,725 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,728 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,732 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,735 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,739 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,742 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,746 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,749 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,753 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,756 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,760 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,763 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,767 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,770 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,774 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,777 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,781 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,784 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,788 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,791 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,794 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,798 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,801 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,805 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,808 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,812 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,815 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,819 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,822 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,826 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,829 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,833 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,836 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,839 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,843 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,847 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,850 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,853 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,857 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,860 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,864 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,867 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,871 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,874 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,878 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,881 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,885 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,888 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,892 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,895 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,899 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,902 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,906 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,909 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,912 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,916 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,919 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,923 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,926 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,930 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,933 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,937 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,940 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,944 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,947 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,951 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,954 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,958 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,961 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,965 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,968 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,971 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,975 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,978 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,982 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,985 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,989 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,992 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,996 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:06,999 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,003 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,006 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,010 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,013 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,016 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,020 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,023 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,027 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,030 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,034 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,037 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,041 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,044 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,048 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,051 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,055 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,058 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,062 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,065 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,069 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,072 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,076 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,079 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,082 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,086 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,089 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,093 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,096 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,100 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,103 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,107 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,110 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,114 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,117 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,121 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,124 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,128 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,131 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,135 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,138 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,141 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,145 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,148 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,152 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,155 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,159 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,162 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,166 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,169 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,173 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,176 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,180 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,183 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,187 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,190 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,193 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,197 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,200 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,204 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,207 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,211 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,214 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,218 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,221 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,225 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,228 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,232 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,235 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,239 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,242 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,245 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,249 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,252 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,256 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,259 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,263 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,266 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,270 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,273 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,277 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,280 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,284 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,287 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,291 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,294 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,298 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,301 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,304 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,308 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,311 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,315 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,318 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,322 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,325 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,329 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,332 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,336 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,339 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,343 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,346 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,349 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,353 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,356 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,360 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,363 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,367 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,370 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,374 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,377 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,381 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,384 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,388 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,391 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,394 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,398 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,401 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,405 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,408 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,412 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,415 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,419 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,422 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,426 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,429 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,433 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,436 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,440 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,443 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,447 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,450 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,454 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,457 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,461 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,464 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,467 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,471 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,474 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,478 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,481 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,485 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,488 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,492 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,495 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,499 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,502 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,506 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,509 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,512 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,516 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,519 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,523 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,526 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,530 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,533 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,537 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,540 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,544 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,547 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,551 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,554 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,558 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,561 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,565 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,568 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,572 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,575 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,578 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,582 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,585 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,589 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,592 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,596 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,599 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,603 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,606 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,610 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,613 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,617 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,620 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,624 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,627 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,630 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,634 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,637 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,641 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,644 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,648 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,651 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,655 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,658 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,662 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,665 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,669 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,672 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,676 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,679 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,682 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,686 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,689 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,693 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,696 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,700 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,703 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,707 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,710 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,714 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,717 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,720 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,724 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,727 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,731 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,734 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,738 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,741 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,745 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,748 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,752 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,755 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,759 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,762 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,766 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,769 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,773 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,776 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,780 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,783 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,786 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,790 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,793 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,797 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,800 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,804 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,807 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,811 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,814 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,818 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,821 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,824 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,828 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,831 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,835 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,838 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,842 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,845 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,849 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,852 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,856 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,859 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,863 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,866 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,869 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,873 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,876 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,880 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,883 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,887 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,890 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,894 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,897 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,901 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,904 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,908 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,911 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,915 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,918 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,921 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,925 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,928 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,932 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,935 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,939 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,942 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,946 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,949 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,953 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,956 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,960 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,963 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,967 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,970 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,973 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,977 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,980 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,984 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,987 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,991 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,994 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:07,998 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,001 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,005 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,008 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,012 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,015 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,018 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,022 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,025 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,029 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,032 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,036 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,039 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,043 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,046 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,050 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,053 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,056 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,060 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,063 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,067 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,070 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,074 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,077 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,081 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,084 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,088 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,091 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,095 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,098 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,102 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,105 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,108 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,112 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,115 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,119 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,122 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,126 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,129 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,133 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,136 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,140 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,143 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,147 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,150 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,154 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,157 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,161 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,164 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,168 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,171 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,174 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,178 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,181 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,185 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,188 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,192 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,195 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,199 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,202 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,206 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,209 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,213 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,216 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,219 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,223 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,226 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,230 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,233 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,237 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,240 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,244 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,247 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,251 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,254 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,258 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,261 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,264 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,268 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,271 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,275 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,278 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,282 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,285 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,289 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,292 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,296 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,299 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,303 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,306 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,309 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,313 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,316 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,320 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,323 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,327 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,330 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,334 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,337 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,341 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,344 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,348 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,351 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,355 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,358 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,362 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,365 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,368 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,372 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,375 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,379 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,382 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,386 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,389 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,393 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,396 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,400 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,403 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,407 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,410 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,413 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,417 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,420 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,424 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,427 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,431 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,434 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,438 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,441 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,445 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,448 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,452 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,455 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,459 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,462 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,465 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,469 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,472 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,476 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,479 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,483 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,486 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,490 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,493 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,497 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,500 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,504 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,507 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,511 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,514 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,517 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,521 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,524 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,528 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,531 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,535 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,538 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,542 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,545 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,549 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,552 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,556 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,559 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,563 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,566 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,569 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,573 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,576 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,580 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,583 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,587 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,590 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,594 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,597 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,601 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,604 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,608 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,611 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,615 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,618 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,621 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,625 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,628 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,632 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,635 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,639 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,642 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,646 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,649 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,653 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,656 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,660 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,663 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,667 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,670 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,673 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,677 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,680 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,684 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,687 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,691 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,694 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,698 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,701 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,705 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,708 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,712 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,715 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,719 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,722 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,725 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,729 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,732 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,736 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,739 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,743 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,746 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,750 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,753 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,757 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,760 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,764 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,767 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,770 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,774 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,777 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,781 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,784 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,788 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,791 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,795 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,798 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,802 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,805 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,808 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,812 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,815 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,819 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,822 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,826 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,829 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,833 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,836 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,840 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,843 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,847 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,850 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,854 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,857 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,860 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,864 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,867 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,871 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,874 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,878 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,881 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,885 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,888 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,892 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,895 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,899 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,902 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,906 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,909 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,912 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,916 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,919 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,923 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,926 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,930 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,933 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,937 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,940 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,944 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,947 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,951 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,954 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,958 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,961 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,964 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,968 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,971 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,975 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,978 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,982 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,985 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,989 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,992 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,996 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:08,999 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,003 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,006 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,010 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,013 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,017 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,020 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,023 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,027 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,030 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,034 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,037 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,041 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,044 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,048 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,051 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,055 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,058 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,062 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,065 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,069 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,072 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,076 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,079 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,082 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,086 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,089 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,093 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,096 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,100 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,103 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,107 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,110 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,114 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,117 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,120 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,124 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,127 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,131 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,134 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,138 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,141 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,145 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,148 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,152 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,155 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,159 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,162 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,166 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,169 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,172 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,176 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,179 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,183 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,186 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,190 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,193 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,197 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,200 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,204 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,207 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,211 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,214 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,218 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,221 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,224 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,228 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,231 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,235 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,238 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,242 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,245 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,249 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,252 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,256 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,259 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,263 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,266 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,270 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,273 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,276 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,280 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,283 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,287 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,290 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,294 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,297 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,301 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,304 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,308 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,311 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,315 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,318 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,322 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,325 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,329 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,332 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,336 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,339 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,343 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,346 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,349 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,353 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,356 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,360 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,363 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,367 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,370 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,374 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,377 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,381 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,384 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,388 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,391 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,395 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,398 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,402 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,405 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,409 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,412 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,416 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,419 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,422 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,426 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,429 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,433 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,436 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,440 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,443 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,447 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,450 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,454 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,457 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,461 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,464 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,468 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,471 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,475 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,478 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,481 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,485 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,488 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,492 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,495 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,499 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,502 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,506 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,509 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,513 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,516 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,520 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,523 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,527 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,530 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,534 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,537 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,541 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,544 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,548 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,551 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,555 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,558 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,561 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,565 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,568 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,572 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,575 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,579 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,582 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,586 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,589 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,593 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,596 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,600 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,603 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,607 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,610 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,613 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,617 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,620 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,624 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,627 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,631 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,634 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,638 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,641 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,645 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,648 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,652 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,655 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,659 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,662 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,666 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,669 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,673 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,676 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,680 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,683 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,686 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,690 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,693 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,697 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,700 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,704 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,707 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,711 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,714 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,718 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,721 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,725 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,728 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,732 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,735 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,738 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,742 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,745 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,749 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,752 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,756 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,759 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,763 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,766 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,770 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,773 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,777 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,780 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,784 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,787 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,791 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,794 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,798 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,801 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,804 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,808 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,811 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,815 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,818 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,822 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,825 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,829 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,832 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,836 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,839 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,843 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,846 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,849 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,853 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,856 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,860 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,863 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,867 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,870 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,874 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,877 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,881 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,884 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,888 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,891 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,895 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,898 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,901 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,905 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,908 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,912 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,915 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,919 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,922 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,926 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,929 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,933 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,936 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,939 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,943 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,946 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,950 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,953 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,957 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,960 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,964 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,967 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,971 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,974 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,978 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,981 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,985 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,988 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,992 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,995 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:09,999 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,002 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,006 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,009 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,012 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,016 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,019 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,023 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,026 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,030 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,033 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,037 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,040 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,044 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,047 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,051 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,054 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,057 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,061 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,064 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,068 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,071 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,075 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,078 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,082 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,085 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,089 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,092 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,096 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,099 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,103 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,106 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,110 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,113 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,117 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,120 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,124 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,127 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,131 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,134 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,137 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,141 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,144 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,148 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,151 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,155 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,158 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,162 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,165 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,169 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,172 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,176 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,179 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,182 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,186 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,189 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,193 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,196 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,200 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,203 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,207 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,210 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,214 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,217 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,221 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,224 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,228 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,231 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,234 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,238 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,241 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,245 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,248 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,252 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,255 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,259 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,262 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,266 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,269 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,273 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,276 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,279 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,283 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,286 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,290 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,293 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,297 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,300 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,304 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,307 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,311 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,314 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,318 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,321 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,324 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,328 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,331 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,335 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,338 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,342 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,345 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,349 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,352 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,356 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,359 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,363 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,366 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,369 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,373 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,376 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,380 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,383 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,387 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,390 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,394 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,397 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,401 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,404 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,408 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,411 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,415 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,418 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,422 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,425 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,428 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,432 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,435 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,439 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,442 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,446 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,449 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,453 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,456 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,460 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,463 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,467 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,470 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,473 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,477 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,480 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,484 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,487 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,491 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,494 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,498 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,501 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,505 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,508 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,512 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,515 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,519 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,522 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,526 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,529 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,532 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,536 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,539 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,543 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,546 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,550 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,553 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,557 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,560 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,564 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,567 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,571 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,574 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,577 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,581 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,584 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,588 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,591 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,595 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,598 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,602 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,605 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,609 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,612 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,616 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,619 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,622 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,626 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,629 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,633 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,636 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,640 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,643 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,647 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,650 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,654 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,657 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,660 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,664 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,667 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,671 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,674 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,678 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,681 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,685 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,688 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,692 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,695 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,699 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,702 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,706 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,709 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,713 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,716 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,719 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,723 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,726 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,730 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,733 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,737 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,740 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,744 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,747 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,751 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,754 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,758 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,761 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,764 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,768 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,771 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,775 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,778 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,782 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,785 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,789 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,792 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,796 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,799 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,803 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,806 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,810 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,813 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,817 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,820 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,824 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,827 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,830 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,834 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,837 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,841 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,844 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,848 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,851 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,855 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,858 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,862 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,865 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,869 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,872 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,876 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,879 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,882 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,886 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,889 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,893 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,896 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,900 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,903 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,907 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,910 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,914 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,917 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,921 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,924 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,928 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,931 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,935 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,938 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,941 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,945 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,948 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,952 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,955 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,959 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,962 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,966 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,969 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,973 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,976 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,980 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,983 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,987 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,990 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,994 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:10,997 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,000 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,004 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,007 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,011 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,014 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,018 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,021 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,025 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,028 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,032 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,035 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,039 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,042 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,045 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,049 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,052 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,056 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,059 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,067 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,070 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,074 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,077 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,081 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,084 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,088 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,091 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,095 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,098 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,102 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,105 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,109 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,112 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,115 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,119 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,122 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,126 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,129 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,133 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,136 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,140 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,143 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,147 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,150 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,154 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,157 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,160 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,164 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,167 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,171 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,174 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,178 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,181 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,185 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,188 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,192 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,195 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,199 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,202 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,205 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,209 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,212 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,216 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,219 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,223 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,226 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,230 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,233 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,237 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,240 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,244 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,247 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,250 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,254 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,257 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,261 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,264 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,268 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,271 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,275 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,278 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,281 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,285 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,288 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,292 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,295 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,299 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,302 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,306 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,309 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,313 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,316 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,320 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,323 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,327 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,330 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,334 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,337 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,340 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,344 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,347 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,351 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,354 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,358 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,361 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,365 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,368 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,372 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,375 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,379 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,382 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,386 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,389 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,393 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,396 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,400 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,403 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,407 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,410 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,413 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,417 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,420 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,424 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,427 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,431 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,434 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,438 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,441 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,445 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,448 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,452 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,455 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,459 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,462 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,466 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,469 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,472 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,476 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,479 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,483 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,486 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,490 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,493 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,497 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,500 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,504 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,507 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,510 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,514 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,517 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,521 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,524 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,528 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,531 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,535 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,538 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,542 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,545 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,549 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,552 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,556 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,559 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,563 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,566 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,570 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,573 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,576 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,580 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,583 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,587 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,590 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,594 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,597 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,601 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,604 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,607 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,611 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,614 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,618 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,621 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,625 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,628 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,632 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,635 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,639 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,642 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,646 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,649 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,652 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,656 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,659 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,663 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,666 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,670 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,673 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,677 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,680 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,684 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,687 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,691 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,694 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,697 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,729 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,733 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,737 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,740 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,744 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,747 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,751 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,754 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,758 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,761 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,764 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,768 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,771 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,775 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,778 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,782 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,785 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,789 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,792 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,796 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,799 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,803 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,806 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,810 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,813 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,817 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,820 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,823 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,827 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,830 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,834 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,837 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,841 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,844 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,848 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,851 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,855 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,858 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,861 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,865 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,868 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,872 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,875 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,879 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,882 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,886 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,889 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,893 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,896 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,899 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,903 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,906 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,910 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,913 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,917 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,920 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,924 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,927 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,931 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,934 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,937 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,941 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,944 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,948 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,951 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,955 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,958 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,962 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,965 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,969 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,972 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,976 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,979 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,982 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,986 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,989 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,993 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:11,996 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,000 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,003 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,007 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,010 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,014 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,017 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,021 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,024 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,028 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,031 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,035 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,038 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,042 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,045 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,049 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,052 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,055 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,059 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,062 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,066 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,069 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,073 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,076 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,080 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,083 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,087 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,090 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,094 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,097 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,101 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,104 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,107 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,111 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,114 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,118 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,121 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,125 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,128 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,132 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,135 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,138 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,142 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,145 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,149 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,152 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,156 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,159 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,163 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,166 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,170 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,173 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,176 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,180 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,183 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,187 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,190 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,194 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,197 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,201 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,204 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,208 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,211 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,215 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,218 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,222 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,225 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,229 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,232 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,236 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,239 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,243 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,246 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,250 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,253 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,257 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,260 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,264 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,267 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,270 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,274 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,277 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,281 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,284 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,288 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,291 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,295 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,298 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,302 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,305 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,309 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,312 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,316 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,319 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,322 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,326 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,329 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,333 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,336 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,340 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,343 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,347 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,350 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,354 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,357 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,361 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,364 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,368 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,371 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,374 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,378 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,381 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,385 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,388 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,392 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,395 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,399 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,402 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,406 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,409 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,413 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,416 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,420 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,423 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,427 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,430 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,434 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,437 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,441 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,444 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,448 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,451 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,455 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,458 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,461 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,465 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,468 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,472 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,475 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,479 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,482 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,486 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,489 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,493 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,496 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,499 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,503 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,506 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,510 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,513 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,517 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,520 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,524 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,527 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,530 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,534 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,537 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,541 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,544 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,548 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,551 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,555 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,558 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,562 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,565 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,568 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,572 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,575 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,579 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,582 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,586 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,589 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,593 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,596 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,600 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,603 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,607 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,610 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,613 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,617 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,620 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,624 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,628 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,631 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,635 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,638 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,642 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,645 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,648 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,652 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,655 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,659 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,662 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,666 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,669 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,673 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,676 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,680 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,683 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,687 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,690 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,693 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,697 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,700 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,704 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,707 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,711 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,714 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,718 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,721 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,725 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,728 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,732 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,735 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,738 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,742 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,745 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,749 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,752 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,756 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,759 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,763 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,766 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,769 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,773 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,776 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,780 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,784 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,787 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,790 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,794 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,797 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,801 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,804 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,807 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,811 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,814 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,818 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,821 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,824 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,828 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,832 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,835 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,839 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,842 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,845 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,849 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,852 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,856 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,859 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,863 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,866 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,869 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,873 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,876 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,880 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,883 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,886 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,890 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,893 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,897 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,900 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,904 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,907 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,910 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,914 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,917 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,921 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,924 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,927 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,931 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,934 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,938 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,941 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,945 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,948 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,951 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,955 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,958 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,962 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,965 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,968 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,972 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,975 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,979 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,982 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,986 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,989 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,992 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,996 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:12,999 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,003 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,006 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,010 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,013 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,016 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,020 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,023 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,027 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,030 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,034 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,037 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,041 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,044 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,048 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,051 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,054 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,058 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,061 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,065 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,068 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,071 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,075 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,078 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,082 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,085 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,089 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,092 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,095 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,099 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,102 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,106 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,109 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,112 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,116 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,119 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,123 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,126 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,130 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,133 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,136 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,140 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,143 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,147 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,150 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,153 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,157 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,160 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,164 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,167 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,170 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,174 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,177 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,181 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,184 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,187 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,191 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,194 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,198 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,201 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,204 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,208 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,211 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,215 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,218 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,222 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,225 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,228 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,232 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,236 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,239 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,242 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,246 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,249 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,253 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,256 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,260 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,263 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,266 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,270 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,273 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,277 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,280 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,283 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,287 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,290 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,294 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,297 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,301 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,304 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,308 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,311 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,315 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,318 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,321 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,325 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,328 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,332 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,335 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,339 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,342 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,345 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,349 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,352 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,356 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,359 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,362 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,366 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,369 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,373 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,376 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,380 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,383 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,386 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,390 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,393 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,397 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,400 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,404 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,407 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,410 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,414 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,417 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,421 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,424 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,427 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,431 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,434 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,438 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,442 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,445 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,448 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,452 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,455 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,459 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,462 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,465 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,469 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,472 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,476 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,479 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,483 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,486 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,489 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,493 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,496 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,500 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,503 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,506 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,510 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,513 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,517 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,520 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,523 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,527 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,530 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,534 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,537 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,541 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,544 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,547 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,551 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,554 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,558 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,561 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,564 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,568 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,571 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,575 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,578 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,581 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,585 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,588 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,592 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,595 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,599 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,602 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,605 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,609 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,612 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,616 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,619 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,622 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,626 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,629 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,633 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,636 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,639 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,643 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,647 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,650 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,653 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,657 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,660 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,664 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,667 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,670 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,674 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,677 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,681 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,684 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,687 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,691 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,694 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,698 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,701 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,705 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,708 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,711 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,715 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,718 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,722 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,725 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,728 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,732 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,735 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,739 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,742 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,746 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,749 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,752 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,756 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,759 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,763 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,766 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,769 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,773 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,776 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,780 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,783 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,786 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,790 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,793 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,797 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,800 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,804 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,807 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,810 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,814 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,817 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,821 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,824 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,827 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,831 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,834 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,838 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,841 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,844 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,848 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,852 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,855 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,858 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,862 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,865 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,869 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,872 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,876 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,879 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,882 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,886 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,889 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,893 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,896 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,899 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,903 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,906 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,910 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,913 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,917 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,920 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,923 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,927 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,930 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,934 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,937 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,940 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,944 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,947 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,951 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,954 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,958 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,961 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,964 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,968 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,971 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,975 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,978 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,981 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,985 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,988 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,992 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,995 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:13,999 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,002 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,005 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,009 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,012 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,016 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,019 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,022 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,026 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,029 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,033 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,036 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,040 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,043 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,046 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,050 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,054 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,057 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,060 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,064 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,067 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,071 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,074 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,077 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,081 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,084 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,088 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,091 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,095 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,098 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,101 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,105 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,108 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,112 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,115 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,118 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,122 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,125 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,129 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,132 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,135 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,139 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,142 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,146 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,149 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,152 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,156 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,159 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,163 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,166 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,170 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,173 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,176 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,180 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,183 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,187 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,190 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,193 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,197 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,200 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,204 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,207 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,210 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,214 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,217 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,221 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,224 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,228 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,231 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,234 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,238 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,241 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,245 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,248 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,251 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,255 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,259 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,262 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,265 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,269 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,272 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,276 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,279 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,282 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,286 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,289 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,293 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,296 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,299 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,303 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,306 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,310 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,313 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,316 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,320 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,323 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,327 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,330 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,334 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,337 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,340 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,344 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,347 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,351 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,354 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,357 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,361 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,364 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,368 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,371 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,375 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,378 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,381 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,385 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,388 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,392 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,395 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,399 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,402 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,405 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,409 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,412 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,416 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,419 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,422 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,426 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,429 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,433 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,436 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,439 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,443 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,446 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,450 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,453 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,457 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,460 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,464 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,467 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,471 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,474 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,477 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,481 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,484 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,488 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,491 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,495 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,498 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,501 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,505 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,508 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,512 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,515 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,518 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,522 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,525 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,529 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,532 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,536 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,539 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,542 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,546 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,549 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,553 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,556 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,559 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,563 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,566 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,570 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,573 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,576 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,580 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,583 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,587 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,590 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,594 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,597 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,600 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,604 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,607 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,611 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,614 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,617 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,621 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,624 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,628 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,631 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,634 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,638 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,641 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,645 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,648 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,651 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,655 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,658 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,662 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,665 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,669 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,672 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,676 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,679 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,683 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,686 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,689 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,693 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,696 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,700 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,703 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,706 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,710 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,713 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,717 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,720 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,723 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,727 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,730 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,734 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,737 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,740 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,744 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,747 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,751 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,754 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,758 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,761 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,764 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,768 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,771 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,775 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,778 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,782 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,785 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,788 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,792 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,795 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,799 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,802 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,805 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,809 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,812 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,816 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,819 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,822 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,826 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,829 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,833 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,836 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,840 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,843 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,846 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,850 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,853 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,857 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,860 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,863 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,867 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,870 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,874 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,877 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,881 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,884 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,887 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,891 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,894 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,898 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,901 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,905 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,908 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,912 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,915 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,919 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,922 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,925 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,929 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,932 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,936 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,939 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,943 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,946 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,950 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,953 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,957 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,960 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,963 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,967 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,970 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,974 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,977 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,981 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,984 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,988 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,991 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,995 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:14,998 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,001 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,005 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,008 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,012 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,015 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,019 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,022 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,026 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,029 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,032 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,036 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,039 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,043 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,046 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,050 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,053 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,057 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,060 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,063 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,067 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,070 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,074 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,078 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,081 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,084 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,088 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,091 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,095 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,098 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,102 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,105 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,109 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,112 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,115 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,119 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,122 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,126 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,129 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,133 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,136 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,140 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,143 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,146 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,150 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,153 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,157 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,160 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,164 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,167 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,171 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,174 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,177 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,181 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,184 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,188 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,191 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,195 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,198 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,202 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,205 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,209 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,212 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,215 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,219 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,222 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,226 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,229 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,233 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,236 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,240 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,243 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,246 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,250 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,253 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,257 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,260 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,264 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,267 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,270 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,274 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,277 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,281 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,285 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,288 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,292 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,295 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,298 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,302 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,305 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,309 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,312 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,316 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,319 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,323 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,326 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,329 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,333 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,336 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,340 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,343 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,347 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,350 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,354 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,357 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,360 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,364 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,367 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,371 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,374 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,378 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,381 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,385 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,388 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,392 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,395 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,398 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,402 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,405 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,409 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,412 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,416 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,419 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,423 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,426 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,430 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,433 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,437 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,440 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,443 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,447 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,450 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,454 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,457 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,461 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,464 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,468 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,471 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,475 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,478 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,481 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,485 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,489 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,492 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,496 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,499 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,503 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,506 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,509 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,513 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,516 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,520 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,523 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,527 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,530 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,533 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,537 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,540 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,544 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,547 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,551 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,554 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,558 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,561 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,565 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,568 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,571 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,575 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,578 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,582 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,585 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,589 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,592 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,595 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,599 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,602 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,606 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,609 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,613 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,616 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,620 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,623 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,627 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,630 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,633 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,637 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,640 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,644 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,647 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,651 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,654 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,658 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,661 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,664 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,668 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,671 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,675 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,678 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,682 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,685 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,689 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,692 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,696 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,699 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,703 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,706 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,710 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,713 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,717 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,720 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,723 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,727 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,730 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,734 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,737 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,741 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,744 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,748 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,751 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,754 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,758 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,761 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,765 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,768 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,772 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,775 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,779 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,782 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,786 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,789 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,792 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,796 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,799 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,803 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,806 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,810 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,813 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,817 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,820 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,823 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,827 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,830 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,834 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,837 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,841 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,844 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,848 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,851 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,855 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,858 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,861 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,865 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,868 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,872 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,875 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,879 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,882 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,886 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,889 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,893 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,896 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,900 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,903 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,907 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,910 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,914 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,917 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,920 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,924 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,927 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,931 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,934 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,938 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,941 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,945 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,948 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,952 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,955 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,958 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,962 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,965 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,969 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,972 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,976 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,979 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,982 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,986 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,989 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,993 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:15,996 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,000 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,003 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,007 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,010 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,013 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,017 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,020 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,024 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,027 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,031 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,034 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,038 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,041 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,044 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,048 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,051 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,055 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,058 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,062 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,065 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,069 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,072 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,075 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,079 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,082 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,086 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,089 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,093 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,096 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,100 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,103 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,106 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,110 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,113 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,117 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,120 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,124 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,127 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,131 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,134 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,137 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,141 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,144 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,148 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,151 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,155 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,158 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,162 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,165 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,168 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,172 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,175 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,179 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,182 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,186 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,189 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,193 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,196 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,200 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,203 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,206 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,210 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,213 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,217 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,220 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,224 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,227 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,231 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,234 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,237 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,241 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,244 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,248 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,251 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,255 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,258 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,262 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,265 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,269 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,272 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,276 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,279 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,282 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,286 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,289 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,293 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,296 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,300 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,303 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,307 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,310 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,313 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,317 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,320 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,324 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,327 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,331 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,334 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,338 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,341 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,345 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,348 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,352 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,355 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,358 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,362 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,365 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,369 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,372 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,376 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,379 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,382 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,386 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,389 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,393 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,396 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,400 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,403 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,407 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,410 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,414 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,417 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,420 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,424 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,427 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,431 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,434 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,438 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,441 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,444 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,448 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,451 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,455 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,458 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,462 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,465 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,469 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,472 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,476 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,479 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,482 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,486 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,489 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,493 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,496 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,500 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,503 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,506 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,510 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,513 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,517 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,520 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,524 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,527 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,531 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,534 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,537 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,541 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,544 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,548 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,551 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,555 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,558 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,562 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,565 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,569 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,572 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,575 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,579 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,582 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,586 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,589 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,593 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,596 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,600 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,603 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,606 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,610 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,613 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,617 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,620 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,624 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,627 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,631 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,634 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,638 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,641 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,644 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,648 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,651 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,655 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,658 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,662 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,665 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,669 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,672 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,675 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,679 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,682 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,686 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,689 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,693 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,696 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,700 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,703 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,706 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,710 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,713 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,717 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,720 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,724 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,727 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,731 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,734 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,737 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,741 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,744 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,748 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,751 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,755 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,758 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,762 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,765 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,768 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,772 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,775 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,779 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,782 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,786 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,789 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,793 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,796 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,800 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,803 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,806 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,810 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,813 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,817 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,820 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,824 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,827 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,831 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,834 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,837 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,841 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,844 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,848 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,851 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,855 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,858 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,862 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,865 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,869 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,872 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,876 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,879 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,882 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,886 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,889 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,893 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,896 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,900 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,903 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,906 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,910 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,913 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,917 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,920 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,924 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,927 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,931 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,934 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,938 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,941 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,945 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,948 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,951 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,955 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,958 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,962 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,965 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,969 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,972 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,976 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,979 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,983 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,986 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,990 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,993 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:16,997 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,000 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,003 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,007 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,010 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,014 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,017 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,021 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,024 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,028 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,031 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,035 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,038 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,042 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,045 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,049 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,052 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,055 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,059 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,062 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,066 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,069 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,073 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,076 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,080 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,083 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,087 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,090 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,094 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,097 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,101 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,104 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,108 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,111 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,114 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,118 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,121 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,125 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,128 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,132 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,135 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,139 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,142 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,146 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,149 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,152 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,156 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,159 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,163 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,166 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,170 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,173 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,177 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,180 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,184 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,187 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,190 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,194 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,197 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,201 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,204 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,208 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,211 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,215 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,218 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,222 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,225 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,228 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,232 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,235 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,239 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,242 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,246 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,249 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,253 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,256 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,260 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,263 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,267 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,270 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,273 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,277 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,280 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,284 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,287 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,291 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,294 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,298 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,301 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,304 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,308 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,311 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,317 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,320 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,323 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,327 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,330 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,334 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,337 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,341 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,344 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,348 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,351 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,355 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,358 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,361 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,365 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,368 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,372 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,375 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,379 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,382 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,386 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,389 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,393 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,396 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,400 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,403 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,406 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,410 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,413 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,417 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,420 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,424 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,427 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,431 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,434 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,437 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,441 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,444 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,448 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,451 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,455 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,458 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,462 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,465 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,468 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,472 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,475 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,479 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,482 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,486 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,489 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,493 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,496 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,500 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,503 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,506 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,510 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,513 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,517 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,520 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,524 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,527 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,531 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,534 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,538 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,541 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,544 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,548 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,551 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,555 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,558 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,562 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,565 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,569 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,572 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,576 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,579 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,582 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,586 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,589 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,593 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,596 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,600 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,603 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,607 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,610 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,613 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,617 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,620 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,624 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,627 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,631 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,634 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,638 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,641 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,644 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,648 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,651 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,655 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,658 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,662 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,665 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,669 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,672 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,675 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,679 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,682 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,686 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,689 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,693 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,696 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,700 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,703 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,707 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,710 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,713 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,717 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,720 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,724 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,727 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,731 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,734 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,738 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,741 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,744 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,748 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,751 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,755 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,758 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,762 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,765 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,769 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,772 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,776 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,779 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,782 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,786 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,789 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,793 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,796 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,800 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,803 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,807 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,810 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,813 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,817 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,820 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,824 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,827 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,831 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,834 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,838 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,841 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,845 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,848 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,851 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,855 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,858 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,862 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,865 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,869 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,872 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,875 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,879 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,882 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,886 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,889 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,893 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,896 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,900 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,903 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,906 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,910 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,913 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,917 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,920 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,924 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,927 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,931 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,934 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,937 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,941 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,944 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,948 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,951 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,955 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,958 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,962 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,965 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,969 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,972 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,975 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,979 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,982 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,986 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,989 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,993 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:17,996 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,000 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,003 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,007 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,010 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,014 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,017 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,020 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,024 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,027 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,031 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,034 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,038 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,041 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,045 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,048 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,052 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,055 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,058 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,062 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,065 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,069 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,072 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,076 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,079 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,083 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,086 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,089 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,093 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,096 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,100 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,103 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,107 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,110 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,114 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,117 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,121 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,124 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,127 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,131 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,134 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,138 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,141 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,145 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,148 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,152 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,155 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,158 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,162 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,165 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,169 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,172 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,176 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,179 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,183 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,186 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,190 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,193 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,196 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,200 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,203 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,207 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,210 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,214 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,217 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,221 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,224 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,227 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,231 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,234 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,238 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,241 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,245 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,248 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,252 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,255 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,258 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,262 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,265 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,269 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,272 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,276 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,279 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,283 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,286 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,290 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,293 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,297 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,300 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,303 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,307 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,311 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,315 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,318 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,322 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,325 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,329 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,332 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,336 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,339 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,343 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,346 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,349 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,353 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,356 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,360 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,363 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,367 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,370 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,373 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,377 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,380 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,384 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,387 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,391 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,394 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,398 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,401 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,404 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,408 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,411 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,415 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,418 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,422 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,425 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,429 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,432 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,435 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,439 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,442 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,446 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,449 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,453 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,456 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,460 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,463 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,467 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,470 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,474 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,477 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,481 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,484 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,487 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,491 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,494 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,498 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,501 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,505 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,508 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,512 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,515 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,519 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,522 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,525 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,529 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,532 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,536 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,539 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,543 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,546 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,550 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,553 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,557 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,560 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,564 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,567 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,570 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,574 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,577 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,581 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,584 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,588 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,591 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,594 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,598 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,601 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,605 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,608 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,612 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,615 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,619 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,622 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,654 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,658 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,661 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,665 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,668 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,672 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,675 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,678 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,682 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,685 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,689 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,692 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,696 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,699 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,703 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,706 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,709 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,713 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,716 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,720 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,723 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,727 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,730 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,734 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,737 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,741 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,744 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,748 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,751 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,754 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,758 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,761 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,765 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,768 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,772 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,775 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,779 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,782 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,785 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,789 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,792 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,796 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,799 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,803 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,806 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,810 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,813 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,817 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,820 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,824 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,827 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,830 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,834 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,837 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,841 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,844 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,848 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,851 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,855 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,858 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,861 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,865 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,868 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,872 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,875 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,879 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,882 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,886 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,889 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,892 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,896 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,899 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,903 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,906 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,910 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,913 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,917 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,920 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,924 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,927 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,930 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,934 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,937 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,941 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,944 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,948 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,951 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,955 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,958 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,961 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,965 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,968 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,972 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,975 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,979 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,982 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,986 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,989 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,993 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:18,996 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,000 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,003 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,006 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,010 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,013 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,017 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,020 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,024 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,027 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,031 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,034 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,038 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,041 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,045 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,048 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,051 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,055 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,058 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,062 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,065 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,069 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,072 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,076 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,079 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,082 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,086 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,089 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,093 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,096 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,100 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,103 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,107 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,110 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,114 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,117 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,121 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,124 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,127 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,131 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,134 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,138 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,141 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,145 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,148 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,152 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,155 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,158 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,162 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,165 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,169 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,172 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,176 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,179 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,183 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,186 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,189 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,193 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,196 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,200 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,203 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,207 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,210 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,214 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,217 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,221 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,224 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,228 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,231 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,235 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,238 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,242 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,245 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,248 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,252 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,255 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,259 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,262 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,266 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,269 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,273 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,276 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,280 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,283 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,287 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,290 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,293 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,297 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,300 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,304 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,307 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,311 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,314 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,318 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,321 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,325 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,328 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,332 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,335 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,338 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,342 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,345 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,349 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,352 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,356 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,359 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,363 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,366 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,370 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,373 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,376 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,380 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,383 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,387 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,390 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,394 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,397 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,401 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,404 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,408 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,411 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,414 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,418 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,421 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,425 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,428 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,432 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,435 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,439 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,442 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,445 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,449 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,452 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,456 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,459 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,463 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,466 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,470 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,473 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,477 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,480 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,484 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,487 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,491 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,494 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,498 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,501 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,505 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,508 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,511 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,515 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,518 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,522 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,525 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,529 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,532 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,536 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,539 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,543 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,546 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,550 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,553 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,556 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,560 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,563 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,567 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,570 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,574 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,577 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,581 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,584 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,588 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,591 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,594 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,598 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,601 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,605 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,608 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,612 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,615 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,619 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,622 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,625 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,629 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,632 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,636 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,639 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,643 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,646 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,650 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,653 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,657 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,660 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,663 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,667 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,670 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,674 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,677 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,681 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,684 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,688 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,691 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,695 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,698 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,702 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,705 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,708 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,712 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,715 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,719 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,722 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,726 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:19,729 - cwa.research.super_weight_analyzer - WARNING - Error computing sample loss: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:42,374 - cwa.core.models - INFO - Loading model on Lambda Labs: microsoft/DialoGPT-small (Size: small)
2025-09-23 20:50:42,427 - cwa.core.models - INFO - Before loading - Lambda GPU 0: Allocated 0.00GB, Reserved 0.00GB
2025-09-23 20:50:42,428 - cwa.core.models - INFO - Before loading - Lambda GPU 1: Allocated 0.00GB, Reserved 0.00GB
2025-09-23 20:50:42,429 - cwa.core.models - INFO - Before loading - Lambda VM CPU Memory: 7.5% used (33.4GB / 442.7GB)
2025-09-23 20:50:42,434 - cwa.core.models - INFO - Lambda Labs GPU detected: NVIDIA H100 80GB HBM3
2025-09-23 20:50:42,434 - cwa.core.models - INFO - Loading tokenizer...
2025-09-23 20:50:42,711 - cwa.core.models - INFO - Multi-GPU Lambda setup detected - using auto device mapping
2025-09-23 20:50:42,712 - cwa.core.models - INFO - Loading model with Lambda Labs optimization: {'low_cpu_mem_usage': True, 'trust_remote_code': False, 'cache_dir': '/tmp/hf_cache', 'torch_dtype': torch.float16, 'device_map': 'auto', 'use_flash_attention_2': True}
2025-09-23 20:50:42,797 - cwa.core.models - WARNING - Failed to load as CausalLM, trying base model: GPT2LMHeadModel.__init__() got an unexpected keyword argument 'use_flash_attention_2'
2025-09-23 20:50:43,276 - cwa.core.models - INFO - After loading - Lambda GPU 0: Allocated 0.12GB, Reserved 0.19GB
2025-09-23 20:50:43,277 - cwa.core.models - INFO - After loading - Lambda GPU 1: Allocated 0.13GB, Reserved 0.15GB
2025-09-23 20:50:43,278 - cwa.core.models - INFO - After loading - Lambda VM CPU Memory: 7.6% used (33.6GB / 442.7GB)
2025-09-23 20:50:43,288 - cwa.core.models - INFO - Lambda Labs model loaded successfully: {'model_type': 'BaseModel', 'model_name': 'microsoft/DialoGPT-small', 'model_size_category': 'small', 'total_parameters': 124439808, 'trainable_parameters': 124439808, 'parameters_in_millions': 124.44, 'architecture': 'gpt2', 'hidden_size': 768, 'num_layers': 12, 'vocab_size': 50257, 'max_position_embeddings': 1024, 'device_placement': 'cuda:0', 'dtype': 'torch.float16', 'quantized': False}
2025-09-23 20:50:43,289 - accelerate.big_modeling - WARNING - You shouldn't move a model that is dispatched using accelerate hooks.
2025-09-23 20:50:43,296 - cwa.theory.information_geometry - INFO - Initialized InformationGeometricAnalyzer on cuda:0
2025-09-23 20:50:43,297 - cwa.research.super_weight_analyzer - INFO - Starting super weight extraction in super_weight_discovery mode
2025-09-23 20:50:43,298 - cwa.research.super_weight_analyzer - INFO - Detected 12 transformer layers in microsoft/DialoGPT-small
2025-09-23 20:50:43,299 - cwa.research.super_weight_analyzer - INFO - Early layer focus: targeting layers [0, 1, 2, 3]
2025-09-23 20:50:43,300 - cwa.research.super_weight_analyzer - INFO - Analyzing 4 target layers: [0, 1, 2, 3]
2025-09-23 20:50:43,301 - cwa.research.super_weight_analyzer - INFO - Monitoring activation magnitudes in target layers
2025-09-23 20:50:43,523 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,532 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,540 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,552 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,555 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,565 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,571 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,627 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,630 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,633 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,645 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,656 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,666 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,677 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,684 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,687 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,693 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,704 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,711 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,714 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,722 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,725 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,734 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,737 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,740 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,743 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,750 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,758 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,761 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,769 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,775 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,778 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,781 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,784 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,787 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,793 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,796 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,799 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,805 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,808 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,815 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,822 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,825 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,828 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,831 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,834 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,837 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,840 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,843 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,853 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,856 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,862 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,868 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,871 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,875 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,878 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,881 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,887 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,890 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,893 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,901 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,904 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,907 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,910 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,917 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,920 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,930 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,933 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,939 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,942 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,948 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,951 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,954 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,958 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,961 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,964 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,967 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:50:43,974 - cwa.research.super_weight_analyzer - WARNING - Error in forward pass: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cuda:1 (when checking argument in method wrapper_CUDA__native_layer_norm)
2025-09-23 20:54:33,483 - cwa.core.models - INFO - Loading model on Lambda Labs: microsoft/DialoGPT-small (Size: small)
2025-09-23 20:54:33,535 - cwa.core.models - INFO - Before loading - Lambda GPU 0: Allocated 0.00GB, Reserved 0.00GB
2025-09-23 20:54:33,536 - cwa.core.models - INFO - Before loading - Lambda VM CPU Memory: 7.6% used (33.4GB / 442.7GB)
2025-09-23 20:54:33,539 - cwa.core.models - INFO - Lambda Labs GPU detected: NVIDIA H100 80GB HBM3
2025-09-23 20:54:33,539 - cwa.core.models - INFO - Loading tokenizer...
2025-09-23 20:54:34,122 - cwa.core.models - INFO - Loading model with Lambda Labs optimization: {'low_cpu_mem_usage': True, 'trust_remote_code': False, 'cache_dir': '/tmp/hf_cache', 'torch_dtype': torch.float16, 'device_map': 'auto', 'use_flash_attention_2': True}
2025-09-23 20:54:34,342 - cwa.core.models - WARNING - Failed to load as CausalLM, trying base model: GPT2LMHeadModel.__init__() got an unexpected keyword argument 'use_flash_attention_2'
2025-09-23 20:54:34,704 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-09-23 20:54:34,785 - cwa.core.models - INFO - After loading - Lambda GPU 0: Allocated 0.24GB, Reserved 0.32GB
2025-09-23 20:54:34,786 - cwa.core.models - INFO - After loading - Lambda VM CPU Memory: 7.6% used (33.6GB / 442.7GB)
2025-09-23 20:54:34,796 - cwa.core.models - INFO - Lambda Labs model loaded successfully: {'model_type': 'BaseModel', 'model_name': 'microsoft/DialoGPT-small', 'model_size_category': 'small', 'total_parameters': 124439808, 'trainable_parameters': 124439808, 'parameters_in_millions': 124.44, 'architecture': 'gpt2', 'hidden_size': 768, 'num_layers': 12, 'vocab_size': 50257, 'max_position_embeddings': 1024, 'device_placement': 'cuda:0', 'dtype': 'torch.float16', 'quantized': False}
2025-09-23 20:54:34,799 - cwa.theory.information_geometry - INFO - Initialized InformationGeometricAnalyzer on cuda:0
2025-09-23 20:54:34,800 - cwa.research.super_weight_analyzer - INFO - Starting super weight extraction in super_weight_discovery mode
2025-09-23 20:54:34,801 - cwa.research.super_weight_analyzer - INFO - Detected 12 transformer layers in microsoft/DialoGPT-small
2025-09-23 20:54:34,801 - cwa.research.super_weight_analyzer - INFO - Early layer focus: targeting layers [0, 1, 2, 3]
2025-09-23 20:54:34,803 - cwa.research.super_weight_analyzer - INFO - Analyzing 4 target layers: [0, 1, 2, 3]
2025-09-23 20:54:34,804 - cwa.research.super_weight_analyzer - INFO - Monitoring activation magnitudes in target layers
2025-09-23 20:54:35,672 - cwa.research.super_weight_analyzer - INFO - Calculating Hessian-based sensitivity scores
2025-09-23 20:55:13,657 - cwa.research.super_weight_analyzer - INFO - Starting ensemble super weight discovery (top 0.001%)
2025-09-23 20:55:13,659 - cwa.research.super_weight_analyzer - INFO - Running Method 1: Activation outlier detection
2025-09-23 20:55:13,663 - cwa.research.super_weight_analyzer - INFO - Running Method 2: Causal intervention analysis
2025-09-23 20:55:13,669 - cwa.research.super_weight_analyzer - WARNING - Causal intervention failed for 0: 0
2025-09-23 20:55:13,670 - cwa.research.super_weight_analyzer - WARNING - Causal intervention failed for 1: 1
2025-09-23 20:55:13,671 - cwa.research.super_weight_analyzer - WARNING - Causal intervention failed for 2: 2
2025-09-23 20:55:13,672 - cwa.research.super_weight_analyzer - WARNING - Causal intervention failed for 3: 3
2025-09-23 20:55:13,672 - cwa.research.super_weight_analyzer - INFO - Running Method 3: Information bottleneck detection
2025-09-23 20:55:13,673 - cwa.theory.information_geometry - INFO - Analyzing information bottlenecks using mutual information
2025-09-23 20:55:18,884 - cwa.theory.information_geometry - INFO - Analyzed information bottlenecks for 112 layers
2025-09-23 20:55:18,885 - cwa.research.super_weight_analyzer - INFO - Running Method 4: Spectral anomaly detection
2025-09-23 20:55:18,886 - cwa.research.super_weight_analyzer - WARNING - Spectral analysis failed for 0: 0
2025-09-23 20:55:18,887 - cwa.research.super_weight_analyzer - WARNING - Spectral analysis failed for 1: 1
2025-09-23 20:55:18,888 - cwa.research.super_weight_analyzer - WARNING - Spectral analysis failed for 2: 2
2025-09-23 20:55:18,889 - cwa.research.super_weight_analyzer - WARNING - Spectral analysis failed for 3: 3
2025-09-23 20:55:18,890 - cwa.research.super_weight_analyzer - INFO - Ensemble voting: 0 high-confidence candidates from 92 total
2025-09-23 20:55:18,891 - cwa.research.super_weight_analyzer - INFO - Ensemble discovery completed. Found 0 high-confidence candidates
2025-09-23 20:55:18,891 - cwa.research.super_weight_analyzer - INFO - Performing research-level analysis
2025-09-23 20:55:18,892 - cwa.research.super_weight_analyzer - INFO - Exporting research data to outputs/results/microsoft_DialoGPT-small/research
2025-09-23 20:55:18,899 - cwa.research.super_weight_analyzer - WARNING - Matplotlib/Seaborn not available, skipping visualizations
2025-09-23 20:55:18,899 - cwa.research.super_weight_analyzer - INFO - Super weight extraction complete. Found 0 critical weights
